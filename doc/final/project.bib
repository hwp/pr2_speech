
@article{vincent_blind_2014,
	title = {From Blind to Guided Audio Source Separation},
	url = {http://hal.inria.fr/hal-00922378},
	abstract = {Audio is a domain where signal separation has long been considered as a fascinating objective, potentially offering a wide range of new possibilities and experiences in professional and personal contexts, by better taking advantage of audio material and finely analyzing complex acoustic scenes. It has thus always been a major area for research in signal separation and an exciting challenge for industrial applications. Starting with blind separation of toy mixtures in the mid 90's, research has progressed up to real-world scenarios today, with applications to speech enhancement and recognition, music editing, {3D} sound rendering, and audio information retrieval, among others. This has mostly been made possible by the development of increasingly informed separation techniques incorporating knowledge about the sources and/or the mixtures at hand. For instance, speech source separation for remote conferencing can benefit from prior knowledge of the room geometry and/or the names of the speakers, while music remastering will exploit instrument characteristics and knowledge of sound engineers mixing habits. After a brief historical account, we provide an overview of recent and ongoing research in this field, illustrating a variety of models and techniques designed so as to guide the audio source separation process towards efficient and robust solutions.},
	urldate = {2014-03-11},
	journal = {{IEEE} Signal Processing Magazine},
	author = {Vincent, Emmanuel and Bertin, Nancy and Gribonval, Rémi and Bimbot, Frédéric},
	month = may,
	year = {2014},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/B2XZSN8M/Vincent et al. - 2014 - From blind to guided audio source separation.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/GWZVPJIK/en.html:text/html}
}

@inproceedings{lee_combining_1998,
	title = {Combining time-delayed decorrelation and {ICA:} towards solving the cocktail party problem},
	volume = {2},
	shorttitle = {Combining time-delayed decorrelation and {ICA}},
	doi = {10.1109/ICASSP.1998.675498},
	abstract = {We present methods to separate blindly mixed signals recorded in a room. The learning algorithm is based on the information maximization in a single layer neural network. We focus on the implementation of the learning algorithm and on issues that arise when separating speakers in room recordings. We used an infomax approach in a feedforward neural network implemented in the frequency domain using the polynomial filter matrix algebra technique. A fast convergence speed was achieved by using a time-delayed decorrelation method as a preprocessing step. Under minimum-phase mixing conditions this preprocessing step was sufficient for the separation of signals. These methods successfully separated a recorded voice with music in the background (cocktail party problem). Finally, we discuss problems that arise in real world recordings and their potential solutions},
	booktitle = {Proceedings of the 1998 {IEEE} International Conference on Acoustics, Speech and Signal Processing, 1998},
	author = {Lee, Te-Won and Ziehe, A. and Orglmeister, R. and Sejnowski, T.},
	month = may,
	year = {1998},
	keywords = {architectural acoustics, audio recording, audio system, blindly mixed signals, cocktail party problem solution, Convergence, convergence of numerical methods, correlation methods, Decorrelation, delays, Disk recording, fast convergence speed, feedforward neural nets, feedforward neural network architecture, Feedforward neural networks, filtering theory, Filters, frequency domain, Frequency domain analysis, frequency-domain synthesis, {ICA}, independent component analysis, information maximization, learning algorithm, learning (artificial intelligence), Matrices, matrix algebra, minimum-phase mixing conditions, music, neural net architecture, Neural networks, polynomial filter matrix algebra, Polynomials, preprocessing step, real world recordings, recorded voice, room recordings, signal separation, single layer neural network, speech processing, time-delayed decorrelation},
	pages = {1249--1252 vol.2},
	file = {IEEE Xplore Abstract Record:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/CXUWPJDN/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/QKC9JVEE/Lee et al. - 1998 - Combining time-delayed decorrelation and ICA towa.pdf:application/pdf}
}

@inproceedings{vincent_blind_2005,
	title = {Blind audio source separation - A critical review},
	url = {http://hal.inria.fr/inria-00545504},
	urldate = {2014-03-11},
	author = {Vincent, Emmanuel},
	year = {2005},
	file = {Snapshot:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/VXKMTZZT/en.html:text/html}
}

@article{ozerov_general_2012,
	title = {A General Flexible Framework for the Handling of Prior Information in Audio Source Separation},
	volume = {20},
	url = {http://hal.archives-ouvertes.fr/hal-00626962},
	abstract = {Most of audio source separation methods are developed for a particular scenario characterized by the number of sources and channels and the characteristics of the sources and the mixing process. In this paper we introduce a general audio source separation framework based on a library of structured source models that enable the incorporation of prior knowledge about each source via user-specifiable constraints. While this framework generalizes several existing audio source separation methods, it also allows to imagine and implement new efficient methods that were not yet reported in the literature. We first introduce the framework by describing the model structure and constraints, explaining its generality, and summarizing its algorithmic implementation using a generalized expectation-maximization algorithm. Finally, we illustrate the above-mentioned capabilities of the framework by applying it in several new and existing configurations to different source separation problems. We have released a software tool named Flexible Audio Source Separation Toolbox ({FASST)} implementing a baseline version of the framework in Matlab.},
	number = {4},
	urldate = {2014-03-12},
	journal = {{IEEE} Transactions on Audio, Speech and Language Processing},
	author = {Ozerov, Alexey and Vincent, Emmanuel and Bimbot, Frédéric},
	month = may,
	year = {2012},
	keywords = {Audio source separation, expectation-maximization, local Gaussian model, nonnegative matrix factorization},
	pages = {1118--1133},
	file = {Full Text PDF:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/PA2RMA27/Ozerov et al. - 2012 - A General Flexible Framework for the Handling of P.pdf:application/pdf;Snapshot:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/E92HFVXA/hal-00626962.html:text/html}
}

@article{souden_multichannel_2013,
	title = {A Multichannel {MMSE-Based} Framework for Speech Source Separation and Noise Reduction},
	volume = {21},
	issn = {1558-7916},
	doi = {10.1109/TASL.2013.2263137},
	abstract = {We propose a new framework for joint multichannel speech source separation and acoustic noise reduction. In this framework, we start by formulating the minimum-mean-square error ({MMSE)-based} solution in the context of multiple simultaneous speakers and background noise, and outline the importance of the estimation of the activities of the speakers. The latter is accurately achieved by introducing a latent variable that takes N+1 possible discrete states for a mixture of N speech signals plus additive noise. Each state characterizes the dominance of one of the N+1 signals. We determine the posterior probability of this latent variable, and show how it plays a twofold role in the {MMSE-based} speech enhancement. First, it allows the extraction of the second order statistics of the noise and each of the speech signals from the noisy data. These statistics are needed to formulate the multichannel Wiener-based filters (including the minimum variance distortionless response). Second, it weighs the outputs of these linear filters to shape the spectral contents of the signals' estimates following the associated target speakers' activities. We use the spatial and spectral cues contained in the multichannel recordings of the sound mixtures to compute the posterior probability of this latent variable. The spatial cue is acquired by using the normalized observation vector whose distribution is well approximated by a Gaussian-mixture-like model, while the spectral cue can be captured by using a pre-trained Gaussian mixture model for the log-spectra of speech. The parameters of the investigated models and the speakers' activities (posterior probabilities of the different states of the latent variable) are estimated via expectation maximization. Experimental results including comparisons with the well-known independent component analysis and masking are provided to demonstrate the efficiency of the proposed framework.},
	number = {9},
	journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
	author = {Souden, M. and Araki, S. and Kinoshita, K. and Nakatani, T. and Sawada, H.},
	month = sep,
	year = {2013},
	keywords = {acoustic noise reduction, background noise, blind source separation, expectation-maximisation algorithm, expectation maximization estimation, Gaussian distribution, Gaussian-mixture-like model, higher order statistics, independent component analysis, joint multichannel speech source separation, latent variable, least mean squares methods, linear filters, log-spectra, microphone arrays, minimum-mean-square error, minimum-mean-square error based solution, minimum variance distortionless response, {MMSE-based} speech enhancement, multichannel {MMSE-based} framework, multichannel recordings, multichannel Wiener-based filters, multiple simultaneous speakers, N+1 possible discrete states, Noise reduction, normalized observation vector, N speech signals plus additive noise mixture, posterior probability, pretrained Gaussian mixture model, probability, second order statistics, signal denoising, signal estimates, sound mixtures, Speech enhancement, speech signals, Wiener filter, Wiener filters},
	pages = {1913--1928},
	file = {IEEE Xplore Abstract Record:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/9ZSSTWQV/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/M58VVUX3/Souden et al. - 2013 - A Multichannel MMSE-Based Framework for Speech Sou.pdf:application/pdf}
}

@incollection{gribonval_sparse_2010,
	title = {Sparse Component Analysis},
	volume = {1},
	booktitle = {Handbook of Blind Source Separation, Independent Component Analysis and Applications},
	publisher = {Academic Press},
	author = {Gribonval, R. and Zibulevsky, M.},
	year = {2010},
	pages = {367--420},
	file = {Gribonval_Zibulevsky_SCA_chapter.pdf:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/KE4F82SW/Gribonval_Zibulevsky_SCA_chapter.pdf:application/pdf}
}

@article{nawab_signal_1983,
	title = {Signal reconstruction from short-time Fourier transform magnitude},
	volume = {31},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1983.1164162},
	abstract = {In this paper, a signal is shown to be uniquely represented by the magnitude of its short-time Fourier transform ({STFT)} under mild restrictions on the signal and the analysis window of the {STFT.} Furthermore, various algorithms are developed which reconstruct signal from appropriate samples of the {STFT} magnitude. Several of the algorithms can also be used to obtain signal estimates from the processed {STFT} magnitude, which generally does not have a valid short-time structure. These algorithms are successfully applied to the time-scale modification and noise reduction problems in speech processing. Finally, the results presented here have similar potential for other application areas, including those with multidimensional signals.},
	number = {4},
	journal = {{IEEE} Transactions on Acoustics, Speech and Signal Processing},
	author = {Nawab, {S.H.} and Quatieri, {T.F.} and Lim, {J.S.}},
	month = aug,
	year = {1983},
	keywords = {Fourier transforms, Image reconstruction, Noise reduction, Phase noise, Signal analysis, Signal processing algorithms, Signal reconstruction, Speech enhancement, speech processing, Speech synthesis},
	pages = {986--998},
	file = {IEEE Xplore Abstract Record:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/I6PSEVHG/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/39XVHV2H/Nawab et al. - 1983 - Signal reconstruction from short-time Fourier tran.pdf:application/pdf}
}

@article{portnoff_time-frequency_1980,
	title = {Time-frequency representation of digital signals and systems based on short-time Fourier analysis},
	volume = {28},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1980.1163359},
	abstract = {This paper develops a representation for discrete-time signals and systems based on short-time Fourier analysis. The short-time Fourier transform and the time-varying frequency response are reviewed as representations for signals and linear time-varying systems. The problems of representing a signal by its short-time Fourier transform and synthesizing a signal from its transform are considered. A new synthesis equation is introduced that is sufficiently general to describe apparently different synthesis methods reported in the literature. It is shown that a class of linear-filtering problems can be represented as the product of the time-varying frequency response of the filter multiplied by the short-time Fourier transform of the input signal. The representation of a signal by samples of its short-time Fourier transform is applied to the linear filtering problem. This representation is of practical significance because there exists a computationally efficient algorithm for implementing such systems. Finally, the methods of fast convolution age considered as special cases of this representation.},
	number = {1},
	journal = {{IEEE} Transactions on Acoustics, Speech and Signal Processing},
	author = {Portnoff, Michael R.},
	month = feb,
	year = {1980},
	keywords = {Differential equations, Discrete Fourier transforms, Fourier transforms, Frequency response, Maximum likelihood detection, Nonlinear filters, Signal analysis, Signal synthesis, Time frequency analysis, Time varying systems},
	pages = {55--69},
	file = {IEEE Xplore Abstract Record:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/AC5MUC4G/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/CIBNQB5X/Portnoff - 1980 - Time-frequency representation of digital signals a.pdf:application/pdf}
}

@article{allen_short_1977,
	title = {Short term spectral analysis, synthesis, and modification by discrete Fourier transform},
	volume = {25},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1977.1162950},
	abstract = {A theory of short term spectral analysis, synthesis, and modification is presented with an attempt at pointing out certain practical and theoretical questions. The methods discussed here are useful in designing filter banks when the filter bank outputs are to be used for synthesis after multiplicative modifications are made to the spectrum.},
	number = {3},
	journal = {{IEEE} Transactions on Acoustics, Speech and Signal Processing},
	author = {Allen, {J.B.}},
	month = jun,
	year = {1977},
	keywords = {Band pass filters, Bandwidth, Channel bank filters, Discrete Fourier transforms, Filter bank, Frequency, Low pass filters, Signal analysis, Signal synthesis, spectral analysis},
	pages = {235--238},
	file = {IEEE Xplore Abstract Record:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/CES83M7U/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/UKMZBX6J/Allen - 1977 - Short term spectral analysis, synthesis, and modif.pdf:application/pdf}
}

@article{crochiere_weighted_1980,
	title = {A weighted overlap-add method of short-time Fourier {analysis/Synthesis}},
	volume = {28},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1980.1163353},
	abstract = {In this correspondence we present a new structure and a simplified interpretation of short-time Fourier synthesis using synthesis windows. We show that this approach can be interpreted as a modification of the overlap-add method where we inverse the Fourier transform and window by the synthesis window prior to overlap-adding. This simplified interpretation results in a more efficient structure for short-time synthesis when a synthesis window is desired. In addition, we show how this structure can be used for analysis/synthesis applications which require different analysis and synthesis rates, such as time compression or expansion.},
	number = {1},
	journal = {{IEEE} Transactions on Acoustics, Speech and Signal Processing},
	author = {Crochiere, {R.E.}},
	month = feb,
	year = {1980},
	keywords = {Band pass filters, Channel bank filters, Discrete Fourier transforms, Filter bank, Fourier transforms, Sampling methods, Signal analysis, Signal synthesis, Speech analysis, Speech synthesis},
	pages = {99--102},
	file = {IEEE Xplore Abstract Record:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/XP6F5Q4M/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/PW7JXMNB/Crochiere - 1980 - A weighted overlap-add method of short-time Fourie.pdf:application/pdf}
}

@article{griffin_signal_1984,
	title = {Signal estimation from modified short-time Fourier transform},
	volume = {32},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1984.1164317},
	abstract = {In this paper, we present an algorithm to estimate a signal from its modified short-time Fourier transform ({STFT).} This algorithm is computationally simple and is obtained by minimizing the mean squared error between the {STFT} of the estimated signal and the modified {STFT.} Using this algorithm, we also develop an iterative algorithm to estimate a signal from its modified {STFT} magnitude. The iterative algorithm is shown to decrease, in each iteration, the mean squared error between the {STFT} magnitude of the estimated signal and the modified {STFT} magnitude. The major computation involved in the iterative algorithm is the discrete Fourier transform ({DFT)} computation, and the algorithm appears to be real-time implementable with current hardware technology. The algorithm developed in this paper has been applied to the time-scale modification of speech. The resulting system generates very high-quality speech, and appears to be better in performance than any existing method.},
	number = {2},
	journal = {{IEEE} Transactions on Acoustics, Speech and Signal Processing},
	author = {Griffin, D. and Lim, {J.S.}},
	month = apr,
	year = {1984},
	keywords = {Degradation, Discrete Fourier transforms, Estimation theory, Fourier transforms, Hardware, Iterative algorithms, Monitoring, Sampling methods, Signal processing, Speech enhancement},
	pages = {236--243},
	file = {IEEE Xplore Abstract Record:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/TJSDJQ63/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/B6KINXNJ/Griffin and Lim - 1984 - Signal estimation from modified short-time Fourier.pdf:application/pdf}
}

@article{politis_bias-corrected_1995,
	title = {{BIAS-CORRECTED} {NONPARAMETRIC} {SPECTRAL} {ESTIMATION}},
	volume = {16},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9892.1995.tb00223.x/abstract},
	number = {1},
	urldate = {2014-03-26},
	journal = {Journal of time series analysis},
	author = {Politis, Dimitris N. and Romano, Joseph P.},
	year = {1995},
	pages = {67–103},
	file = {tsa3ez.pdf:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/FKATHSH7/tsa3ez.pdf:application/pdf}
}

@techreport{heinzel_spectrum_2002,
	title = {Spectrum and spectral density estimation by the Discrete Fourier transform ({DFT)}, including a comprehensive list of window functions and some new flat-top windows},
	url = {http://helio.estec.esa.int/SP/LISAPATHFINDER/docs/Data_Analysis/GH_FFT.pdf},
	urldate = {2014-03-26},
	institution = {Max Planck Institute},
	author = {Heinzel, Gerhard and Rüdiger, A. and Schilling, Roland},
	year = {2002},
	pages = {122},
	file = {GH_FFT.pdf:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/B6IB6R5W/GH_FFT.pdf:application/pdf}
}

@article{boll_suppression_1979,
	title = {Suppression of acoustic noise in speech using spectral subtraction},
	volume = {27},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1163209},
	number = {2},
	urldate = {2014-04-01},
	journal = {Acoustics, Speech and Signal Processing, {IEEE} Transactions on},
	author = {Boll, Steven},
	year = {1979},
	pages = {113–120},
	file = {01163209.pdf:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/4DQ2WVMH/01163209.pdf:application/pdf}
}

@phdthesis{ince_ego_2011,
	title = {Ego Noise Estimation for Robot Audition},
	abstract = {Robots should listen to their surrounding world by the microphones embedded in their bodies to recognize and understand the auditory environment. This artificial listening capability called robot audition is an important function to understand the surrounding auditory world including sounds such as human voices, music, and other environmental sounds. Robot audition can be improved by incorporating another modality, robot motion, so that the framework is extended to active robot audition. In that sense, active audition can be considered as the first step towards endowing the robot with intelligent behavior. It provides the robot with a processing architecture that will allow it to learn and reason about how to behave in response to complex acoustic environments and conditions. The most important problem encountered in the active audition domain is ego noise, which can be described as the robot’s own noise generated during a motion of the robot. However, it cannot be solved effectively with conventional methods proposed in other signal processing domains. The basic problem with ego noise, like all types of noise in a robot audition system, is that it causes the Signal-to-Noise Ratio ({SNR)} to drop and it contaminates the spectrum of the recorded signal so that it is almost impossible to perform the fundamental applications of robot audition, such as Sound Source Localiza- tion ({SSL)}, Sound Source Separation ({SSS)} and Automatic Speech Recognition ({ASR)}, accurately. Because the complexity of the ego noise is enhanced by the number of motors in action, the negative effects of ego noise are even more severe for a moving robot with many degrees of freedom. This thesis addresses the estimation problem of the ego noise of a robot in order to suppress it for various tasks. The aim of this thesis is to establish a real-time and online ego noise estimation system. To develop a framework for estimating ego noise and to integrate it into the general robot audition framework effectively, we have to consider the following three issues: (1) modeling the process of ego noise estimation, (2) online processing and (3) general applicability of our ego noise estimation method for robot audition. In order to address the modeling issue of ego noise estimation, we first have to resolve three important sub-issues we have determined: Knowledge gathering issue, representa- tion issue and algorithm issue. The templates are good representations of motor noise when the same actions are performed over and over again. We model the ego noise using templates by associating discrete time series data representing the motion (i.e., the angular status of each joint of the robot) with another series of discrete time data representing the ego noise spectrum. The data is stored in a database so that later it can be estimated instantaneously. However, the necessity of offline training poses strict constraints. The new “online” scheme can distinguish between stationary noise (i.e., static fan noise, hardware noise of the robot and possibly changing background noise) and non-stationary ego-motion noise and treat both of them in separate processes. Furthermore, the proposed online training of the templates makes template-based noise estimation method more adequate to real-world applications because it can learn the ego noise of unknown motions on the fly. Whereas the proposed “template learning” mechanism can discriminate the new data entries from the existing templates in the database, the “template update” mechanism adaptively sustains the accuracy and precision of the templates. It also prevents the rapid growth of the size of database. The final issue is the confirmation of general applicability and compliance of the proposed ego noise estimation method on several robot audition applications. The established frameworks for ego noise reduction, noise robust feature extraction, {ASR} and {SSL} are presented. In Chapter 1, we introduce our motivation, our goals, and the technical issues for this study. The problems and requirements for robot audition are explained, and we give the appropriate approaches to these issues. Chapter 2 surveys the literature related to robot audition and signal processing. Since there are different noise sources in a robot environment and ego noise is strongly intertwined with all of them, our robot audition framework has diverse noise processing blocks. We explain the basic methods used in these blocks in a detailed way as existing work. The properties of all noise sources are explained along with a detailed analysis of the noise signals and robot motions. Also, related work is summarized in this chapter. We describe the technical differences between our approaches and conventional ones. In Chapter 3, after specifying general criteria to be able to choose the optimal estimation process for each noise type, we explain how to approach the modeling process of ego noise estimation specifically. Later on, we propose an estimation method called parameterized template estimation. The performance of this original method is compared with those of existing single-channel noise estimation methods. Chapter 4 describes the developments we made on the basic parameterized template estimation system so that it runs online. In order to cope with changing environmental noise, we modify the abstract template concept to our needs. We generate the templates in a way that they only represent the non-stationary noise. The stationary portion of the ego noise with ambient noise is dealt with by a stationary noise estimation method. We explain the details of this unified framework for noise estimation. Moreover, we eliminate the necessity of human intervention in the training procedure by introducing an incremental template learning scheme. Finally, we evaluate the performance of the proposed methods in terms of estimation quality and noise reduction accuracy by using objective performance criteria and discuss the results. Chapter 5 delves into the question of how to suppress the whole-body motion noise of a robot more robustly. For this purpose we integrate template-based ego noise estimation with the already established works from the multi-channel noise reduction literature. Microphone array-based sound source separation is adequate to cancel motor noise with certain spatial properties, thus the performance of this hybrid noise reduction system exceeds the individual performances of the template estimation and multi-channel noise reduction methods. In this chapter, we discuss the implementation and its evaluation in terms of {ASR} accuracy. Chapter 6 describes Missing Feature Theory ({MFT)-based} integration of ego noise reduction and {ASR.} We focus on two different {ASR} systems: single-talker {ASR} and multi-talker {ASR.} Both systems rely on the single-channel and multi-channel noise reduction methods to generate spectro-temporal masks filtering the unreliable acoustic features. We present detailed results regarding recognition accuracy to determine optimal parameters of the mask generation process for each system. In Chapter 7, we provide an extended version of the parameterized template estimation to operate on multi-channel audio data. This feature enables an Sound Source Localization ({SSL)} scheme to whiten the ego noise allowing to eliminate its interfering effect on the the spatio-temporal plane of Multiple {SIgnal} Classification ({MUSIC)} method for {SSL.} We assess the performance in terms of localization accuracy and peak detection rates for {MUSIC.} Chapter 8 outlines the contributions of this thesis and gives an insight into the remaining issues and future work. Chapter 9 summarizes and concludes this dissertation.},
	school = {Tokyo Institute of Technology},
	author = {Ince, Gokhan},
	month = sep,
	year = {2011},
	keywords = {·, array, audition, Automatic, Ego, feature, Microphone, Missing, noise, recognition, reduction, Robot, speech, theory}
}

@inproceedings{ince_ego_2009,
	title = {Ego noise suppression of a robot using template subtraction},
	doi = {10.1109/IROS.2009.5354651},
	abstract = {While a robot is moving, the joints inevitably generate noise due to its motors, i.e. ego-motion noise. This problem is very crucial, especially in humanoid robots, because it tends to have a lot of joints and the motors are located closer to the microphones than the sound sources. In this work, we investigate methods for the prediction and suppression of the ego-motion noise. In the first part, we analyze the performance of different noise subtraction strategies, assuming that the noise prediction problem has been solved. In the second part, we present some results for a noise prediction scheme based on the current robot joint status. Performance is evaluated for a number of criteria, including Automatic Speech Recognition ({ASR).} We demonstrate that our method improves recognition performance during ego-motion considerably.},
	booktitle = {{IEEE/RSJ} International Conference on Intelligent Robots and Systems, 2009. {IROS} 2009},
	author = {Ince, G. and Nakadai, K. and Rodemann, T. and Hasegawa, Y. and Tsujino, H. and Imura, J.},
	month = oct,
	year = {2009},
	keywords = {Acoustic noise, automatic speech recognition, Databases, ego motion noise, ego noise suppression, humanoid robots, intelligent robots, Micromotors, microphone arrays, Neural networks, noise abatement, Noise cancellation, Noise generators, noise prediction problem, noise subtraction, Robotics and automation, Speech recognition, template subtraction},
	pages = {199--204},
	file = {IEEE Xplore Abstract Record:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/K58QV5IP/login.html:text/html;IEEE Xplore Full Text PDF:/home/hwp/.mozilla/firefox/1pw129hc.default-1394317684004/zotero/storage/V73GVTPQ/Ince et al. - 2009 - Ego noise suppression of a robot using template su.pdf:application/pdf}
}

@incollection{vaseghi_spectral_2001,
	title = {Spectral Subtraction},
	isbn = {9780470841624},
	url = {http://dx.doi.org/10.1002/0470841621.ch11},
	booktitle = {Advanced Digital Signal Processing and Noise Reduction},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Vaseghi, Saeed V.},
	year = {2001},
	keywords = {{FFT}, Fourier, magnitude, musical tones, power, spectral subtraction},
	pages = {333–354}
}